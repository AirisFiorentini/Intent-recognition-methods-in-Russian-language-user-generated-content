{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":585165,"sourceType":"datasetVersion","datasetId":284285},{"sourceId":8227443,"sourceType":"datasetVersion","datasetId":4878620}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-01T14:28:39.627236Z","iopub.execute_input":"2024-06-01T14:28:39.628177Z","iopub.status.idle":"2024-06-01T14:28:40.030393Z","shell.execute_reply.started":"2024-06-01T14:28:39.628128Z","shell.execute_reply":"2024-06-01T14:28:40.029436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:40.032093Z","iopub.execute_input":"2024-06-01T14:28:40.032490Z","iopub.status.idle":"2024-06-01T14:28:44.181452Z","shell.execute_reply.started":"2024-06-01T14:28:40.032465Z","shell.execute_reply":"2024-06-01T14:28:44.180376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch import nn\nimport timeit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.182763Z","iopub.execute_input":"2024-06-01T14:28:44.183231Z","iopub.status.idle":"2024-06-01T14:28:44.582960Z","shell.execute_reply.started":"2024-06-01T14:28:44.183202Z","shell.execute_reply":"2024-06-01T14:28:44.581967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Путь к файлу CSV\ndata_path = \"/kaggle/input/atis-airlinetravelinformationsystem/atis_intents.csv\"\ntrain_data_path = \"/kaggle/input/atis-airlinetravelinformationsystem/atis_intents_train.csv\"\ntest_data_path = \"/kaggle/input/atis-airlinetravelinformationsystem/atis_intents_test.csv\"\n\n# Чтение данных из CSV-файлов\ndata = pd.read_csv(data_path)\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\n\ndata = data.rename(columns={'atis_flight': 'intent', ' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning': 'text'})\nlost_intent = {'intent': 'atis_flight', 'text': 'i want to fly from boston at 838 am and arrive in denver at 1110 in the morning'}\ndata = pd.concat([data, pd.DataFrame([lost_intent])], ignore_index=True)\n\n# Find classes with only one sample\nclass_counts = data['intent'].value_counts()\nsingle_sample_classes = class_counts[class_counts == 1].index.tolist()\n\n# Separate single sample classes\nsingle_sample_data = data[data['intent'].isin(single_sample_classes)]\nmultiple_sample_data = data[~data['intent'].isin(single_sample_classes)]\n\n# train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['intent'])\n# Perform stratified split on multiple sample data\ntrain_multiple, test_multiple = train_test_split(multiple_sample_data, test_size=0.2, random_state=42, stratify=multiple_sample_data['intent'])\n\n# Add single sample data to both train and test sets to ensure all classes are present\ntrain = pd.concat([train_multiple, single_sample_data]).reset_index(drop=True)\ntest = pd.concat([test_multiple, single_sample_data]).reset_index(drop=True)\n\n# Ensure no duplicates in train and test sets\ntrain = train.drop_duplicates().reset_index(drop=True)\ntest = test.drop_duplicates().reset_index(drop=True)\n\nunique_values_normalized = train['intent'].value_counts(normalize=True)\nunique_values_normalized_test = test['intent'].value_counts(normalize=True)\nprint(unique_values_normalized*100, len(unique_values_normalized), len(unique_values_normalized_test))\n\n# Check for missing classes and ensure all classes are present\ntrain_classes = set(train['intent'])\ntest_classes = set(test['intent'])\nall_classes = set(data['intent'])\n\n# Find missing classes in train and test sets\nmissing_train_classes = all_classes - train_classes\nmissing_test_classes = all_classes - test_classes\n\n# Add missing classes examples to train and test sets\nif missing_train_classes:\n    missing_train_data = data[data['intent'].isin(missing_train_classes)]\n    train = pd.concat([train, missing_train_data])\n    train = train.drop_duplicates().reset_index(drop=True)\n\nif missing_test_classes:\n    missing_test_data = data[data['intent'].isin(missing_test_classes)]\n    test = pd.concat([test, missing_test_data])\n    test = test.drop_duplicates().reset_index(drop=True)\n\n# Print unique value proportions\nunique_values_normalized = train['intent'].value_counts(normalize=True)\nunique_values_normalized_test = test['intent'].value_counts(normalize=True)\nprint(unique_values_normalized * 100, len(unique_values_normalized), len(unique_values_normalized_test)) ","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.584664Z","iopub.execute_input":"2024-06-01T14:28:44.585083Z","iopub.status.idle":"2024-06-01T14:28:44.656263Z","shell.execute_reply.started":"2024-06-01T14:28:44.585048Z","shell.execute_reply":"2024-06-01T14:28:44.655255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Путь к файлу CSV\n# train_data_path = \"/kaggle/input/snips-dataset/train_data.csv\"\n# validate_data_path = \"/kaggle/input/snips-dataset/validate_data.csv\"\n\n# # Чтение данных из CSV-файлов\n# train_data = pd.read_csv(train_data_path)\n# validate_data = pd.read_csv(validate_data_path)\n\n# # Вывод первых 5 строк каждого DataFrame\n# print(\"Train data:\")\n# print(train_data.head())\n# print(\"\\nValidate data:\")\n# print(validate_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.660152Z","iopub.execute_input":"2024-06-01T14:28:44.660453Z","iopub.status.idle":"2024-06-01T14:28:44.665222Z","shell.execute_reply.started":"2024-06-01T14:28:44.660428Z","shell.execute_reply":"2024-06-01T14:28:44.664085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unique_values_normalized = train_data['intent'].value_counts(normalize=True)\n# print(unique_values_normalized)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.666395Z","iopub.execute_input":"2024-06-01T14:28:44.666679Z","iopub.status.idle":"2024-06-01T14:28:44.679352Z","shell.execute_reply.started":"2024-06-01T14:28:44.666655Z","shell.execute_reply":"2024-06-01T14:28:44.678480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = train_data.drop('entity', axis=1)\n# validate_data = validate_data.drop('entity', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.680449Z","iopub.execute_input":"2024-06-01T14:28:44.680779Z","iopub.status.idle":"2024-06-01T14:28:44.689585Z","shell.execute_reply.started":"2024-06-01T14:28:44.680748Z","shell.execute_reply":"2024-06-01T14:28:44.688706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Удалите строки, где столбец 'column_name' пуст\n# train_data = train_data.dropna(subset=['text'])\n# validate_data = validate_data.dropna(subset=['text'])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.690776Z","iopub.execute_input":"2024-06-01T14:28:44.691156Z","iopub.status.idle":"2024-06-01T14:28:44.699869Z","shell.execute_reply.started":"2024-06-01T14:28:44.691122Z","shell.execute_reply":"2024-06-01T14:28:44.698954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Подсчитываем количество примеров для каждого класса\n# class_counts = train_data['intent'].value_counts()\n\n# # Находим класс с наименьшим количеством примеров\n# min_class = class_counts.idxmin()\n# min_class_count = class_counts.min()\n\n# # Делаем undersampling для всех остальных классов\n# balanced_data = train_data[train_data['intent'] == min_class]\n# for intent in class_counts.index:\n#     if intent != min_class:\n#         samples = train_data[train_data['intent'] == intent].sample(min_class_count)\n#         balanced_data = pd.concat([balanced_data, samples], axis=0)\n\n# # Теперь в balanced_data примеры каждого класса представлены в одинаковом количестве","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.701182Z","iopub.execute_input":"2024-06-01T14:28:44.701818Z","iopub.status.idle":"2024-06-01T14:28:44.710578Z","shell.execute_reply.started":"2024-06-01T14:28:44.701792Z","shell.execute_reply":"2024-06-01T14:28:44.709734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# balanced_data = train_data","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.711765Z","iopub.execute_input":"2024-06-01T14:28:44.712193Z","iopub.status.idle":"2024-06-01T14:28:44.720413Z","shell.execute_reply.started":"2024-06-01T14:28:44.712162Z","shell.execute_reply":"2024-06-01T14:28:44.719530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# balanced_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.721415Z","iopub.execute_input":"2024-06-01T14:28:44.721730Z","iopub.status.idle":"2024-06-01T14:28:44.730683Z","shell.execute_reply.started":"2024-06-01T14:28:44.721699Z","shell.execute_reply":"2024-06-01T14:28:44.729943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.731631Z","iopub.execute_input":"2024-06-01T14:28:44.731890Z","iopub.status.idle":"2024-06-01T14:28:44.933874Z","shell.execute_reply.started":"2024-06-01T14:28:44.731868Z","shell.execute_reply":"2024-06-01T14:28:44.932779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.935170Z","iopub.execute_input":"2024-06-01T14:28:44.935453Z","iopub.status.idle":"2024-06-01T14:28:44.939512Z","shell.execute_reply.started":"2024-06-01T14:28:44.935431Z","shell.execute_reply":"2024-06-01T14:28:44.938627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IntentClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.943437Z","iopub.execute_input":"2024-06-01T14:28:44.944177Z","iopub.status.idle":"2024-06-01T14:28:44.951573Z","shell.execute_reply.started":"2024-06-01T14:28:44.944146Z","shell.execute_reply":"2024-06-01T14:28:44.950737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=2, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.952630Z","iopub.execute_input":"2024-06-01T14:28:44.952919Z","iopub.status.idle":"2024-06-01T14:28:44.966926Z","shell.execute_reply.started":"2024-06-01T14:28:44.952879Z","shell.execute_reply":"2024-06-01T14:28:44.966057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import DistilBertModel\n\nclass CustomDistilBertModel(nn.Module):\n    def __init__(self, num_labels):\n        super(CustomDistilBertModel, self).__init__()\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.dropout = nn.Dropout(0.1)\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, num_labels)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = output[0][:, 0]\n        output = self.dropout(pooled_output)\n        output = self.fc1(output)\n        output = nn.Tanh()(output)\n        output = self.dropout(output)\n        output = self.fc2(output)\n        return output\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.967966Z","iopub.execute_input":"2024-06-01T14:28:44.968255Z","iopub.status.idle":"2024-06-01T14:28:44.986044Z","shell.execute_reply.started":"2024-06-01T14:28:44.968232Z","shell.execute_reply":"2024-06-01T14:28:44.985260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Преобразование меток в числовой формат\nunique_intents = train['intent'].unique().tolist()\nintent_mapping = {intent: i for i, intent in enumerate(unique_intents)}\ntrain_labels = [intent_mapping[intent] for intent in train['intent']]\ntest_labels = [intent_mapping[intent] for intent in test['intent']]","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.987315Z","iopub.execute_input":"2024-06-01T14:28:44.987699Z","iopub.status.idle":"2024-06-01T14:28:44.995426Z","shell.execute_reply.started":"2024-06-01T14:28:44.987669Z","shell.execute_reply":"2024-06-01T14:28:44.994658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создание наборов данных PyTorch\nmax_len = 128\nbatch_size = 16 #16\ntrain_dataset = IntentClassificationDataset(train['text'].tolist(), train_labels, tokenizer, max_len)\ntest_dataset = IntentClassificationDataset(test['text'].tolist(), test_labels, tokenizer, max_len)\n\ntrain_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:44.996440Z","iopub.execute_input":"2024-06-01T14:28:44.996692Z","iopub.status.idle":"2024-06-01T14:28:45.009519Z","shell.execute_reply.started":"2024-06-01T14:28:44.996670Z","shell.execute_reply":"2024-06-01T14:28:45.008613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, AdamW\nimport torch\n\n# Загрузка модели DistilBERT\n# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(unique_intents))\nmodel = CustomDistilBertModel(num_labels=len(unique_intents))\n\n# Компиляция модели\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:45.010657Z","iopub.execute_input":"2024-06-01T14:28:45.010947Z","iopub.status.idle":"2024-06-01T14:28:45.681236Z","shell.execute_reply.started":"2024-06-01T14:28:45.010918Z","shell.execute_reply":"2024-06-01T14:28:45.680438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создание экземпляра класса EarlyStopping\nearly_stopping = EarlyStopping(patience=5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:45.682399Z","iopub.execute_input":"2024-06-01T14:28:45.682712Z","iopub.status.idle":"2024-06-01T14:28:45.687182Z","shell.execute_reply.started":"2024-06-01T14:28:45.682686Z","shell.execute_reply":"2024-06-01T14:28:45.685962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Замораживаем все параметры модели\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# # Размораживаем параметры последнего слоя\n# for param in model.classifier.parameters():\n#     param.requires_grad = True\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:45.688324Z","iopub.execute_input":"2024-06-01T14:28:45.688612Z","iopub.status.idle":"2024-06-01T14:28:45.697423Z","shell.execute_reply.started":"2024-06-01T14:28:45.688589Z","shell.execute_reply":"2024-06-01T14:28:45.696453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\n# from torch.nn import CrossEntropyLoss","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:45.698460Z","iopub.execute_input":"2024-06-01T14:28:45.698750Z","iopub.status.idle":"2024-06-01T14:28:45.707964Z","shell.execute_reply.started":"2024-06-01T14:28:45.698726Z","shell.execute_reply":"2024-06-01T14:28:45.707128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Количество эпох\nn_epochs = 10\n\n# Списки для хранения значений потерь и метрик на каждой эпохе\nloss_values = []\nval_loss_values = []  # список для потерь валидации\nacc_values = []\nrec_values = []\nf1_values = []\n\n\nfor epoch in range(n_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Вызов модели без аргумента labels\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Вычисление потерь\n        loss_fct = CrossEntropyLoss()\n        loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n        \n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    avg_train_loss = total_loss / len(train_data_loader)\n    loss_values.append(avg_train_loss)\n\n    print(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n\n    model.eval()\n    val_loss = 0\n    predictions , true_labels = [], []\n\n    for batch in tqdm(test_data_loader, desc=f\"Validation Epoch {epoch+1}\"):\n        with torch.no_grad():\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Вызов модели без аргумента labels\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n            # Вычисление потерь\n            loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n            \n            val_loss += loss.item()\n\n            logits = outputs.detach().cpu().numpy()\n            label_ids = labels.to('cpu').numpy()\n            \n            predictions.extend(np.argmax(logits, axis=1).flatten())\n            true_labels.extend(label_ids.flatten())\n\n    \n    avg_val_loss = val_loss / len(test_data_loader)\n    val_loss_values.append(avg_val_loss)  # сохраняем потери валидации\n    print('Average validation loss: ', avg_val_loss)\n\n    early_stopping(avg_val_loss, model)\n\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n\n        \n # Построение графика потерь\nplt.plot(range(1, n_epochs+1), loss_values, label='Train Loss')\nplt.plot(range(1, n_epochs+1), val_loss_values, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Построение графиков потерь и метрик\n# plt.figure(figsize=(15,8))\n# plt.title(\"Training and Validation Metrics\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Metric Value\")\n\n# plt.plot(loss_values, label='Train Loss')\n# plt.plot(val_loss_values, label='Validation Loss')  # добавляем потери валидации на график\n# plt.legend()\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:28:45.709035Z","iopub.execute_input":"2024-06-01T14:28:45.709330Z","iopub.status.idle":"2024-06-01T14:33:08.196060Z","shell.execute_reply.started":"2024-06-01T14:28:45.709299Z","shell.execute_reply":"2024-06-01T14:33:08.194976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new\n# from transformers import BertForSequenceClassification\n# import matplotlib.pyplot as plt\n\n# # tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n# # model = BertForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny2\")\n\n# # Количество эпох\n# epochs = 50\n\n# # Список для хранения значений потерь на каждой эпохе\n# loss_values = []\n\n# for epoch in range(epochs):\n#     model.train()\n#     total_loss = 0\n\n#     for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch+1}\"):\n#         optimizer.zero_grad()\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"labels\"].to(device)\n\n#         # Вызов модели без аргумента labels\n#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n#         # Вычисление потерь\n#         loss_fct = CrossEntropyLoss()\n#         loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n        \n#         total_loss += loss.item()\n#         loss.backward()\n#         optimizer.step()\n\n#     avg_train_loss = total_loss / len(train_data_loader)\n#     loss_values.append(avg_train_loss)\n\n#     print(f\"Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n\n#     model.eval()\n#     val_loss = 0\n#     predictions , true_labels = [], []\n\n#     for batch in tqdm(test_data_loader, desc=f\"Validation Epoch {epoch+1}\"):\n#         with torch.no_grad():\n#             input_ids = batch[\"input_ids\"].to(device)\n#             attention_mask = batch[\"attention_mask\"].to(device)\n#             labels = batch[\"labels\"].to(device)\n\n#             # Вызов модели без аргумента labels\n#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n#             # Вычисление потерь\n#             loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n            \n#             val_loss += loss.item()\n\n#     avg_val_loss = val_loss / len(test_data_loader)\n#     print('Average validation loss: ', avg_val_loss)\n\n#     early_stopping(avg_val_loss, model)\n\n#     if early_stopping.early_stop:\n#         print(\"Early stopping\")\n#         break\n\n# # Построение графика потерь\n# plt.figure(figsize=(15,8))\n# plt.title(\"Training loss\")\n# plt.xlabel(\"Batch\")\n# plt.ylabel(\"Loss\")\n# plt.plot(loss_values)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:08.197506Z","iopub.execute_input":"2024-06-01T14:33:08.197895Z","iopub.status.idle":"2024-06-01T14:33:08.205284Z","shell.execute_reply.started":"2024-06-01T14:33:08.197858Z","shell.execute_reply":"2024-06-01T14:33:08.204266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# old working\n\n# # Обучение модели\n# for epoch in range(50):\n#     model.train()\n#     total_loss = 0\n#     progress_bar = tqdm(train_data_loader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n#     for batch in progress_bar:\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"labels\"].to(device)\n\n#         # Вызов модели\n#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n#         # Вычисление потерь\n#         loss_fct = CrossEntropyLoss()\n#         loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n        \n#         total_loss += loss.item()\n\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n\n#         # Обновление шкалы выполнения\n#         progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n\n#     # Расчет среднего значения потерь за эпоху\n#     avg_train_loss = total_loss / len(train_data_loader)\n#     print('Average training loss: ', avg_train_loss)\n\n#     # Валидация на независимом наборе данных\n#     model.eval()\n#     val_loss = 0\n#     with torch.no_grad():\n#         for batch in test_data_loader:\n#             input_ids = batch[\"input_ids\"].to(device)\n#             attention_mask = batch[\"attention_mask\"].to(device)\n#             labels = batch[\"labels\"].to(device)\n\n#             # Вызов модели\n#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n#             # Вычисление потерь\n#             loss = loss_fct(outputs.view(-1, len(unique_intents)), labels.view(-1))\n            \n#             val_loss += loss.item()\n\n#     avg_val_loss = val_loss / len(test_data_loader)\n#     print('Average validation loss: ', avg_val_loss)\n\n#     early_stopping(avg_val_loss, model)\n\n#     if early_stopping.early_stop:\n#         print(\"Early stopping\")\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:08.206567Z","iopub.execute_input":"2024-06-01T14:33:08.206844Z","iopub.status.idle":"2024-06-01T14:33:08.224921Z","shell.execute_reply.started":"2024-06-01T14:33:08.206820Z","shell.execute_reply":"2024-06-01T14:33:08.223958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Предсказание на тестовых данных\nmodel.eval()\npredictions = []\nstart_test = timeit.default_timer()\nwith torch.no_grad():\n    for batch in test_data_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n\n        predictions.extend(preds)\nend_test = timeit.default_timer()\n\npredicted_labels = [pred.item() for pred in predictions]\n\n# Общее количество предсказанных ответов\nnum_predictions = len(predictions)\n\n# Среднее время на один ответ\naverage_time_per_response = (end_test - start_test) / num_predictions\n\nprint(f'Time for testing: {end_test - start_test:.4f} seconds')\nprint(f'Average time per response: {average_time_per_response:.6f} seconds')\nprint(balanced_accuracy_score(test_labels, predicted_labels))\nprint(precision_recall_fscore_support(test_labels, predicted_labels, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:08.226020Z","iopub.execute_input":"2024-06-01T14:33:08.226277Z","iopub.status.idle":"2024-06-01T14:33:10.018889Z","shell.execute_reply.started":"2024-06-01T14:33:08.226254Z","shell.execute_reply":"2024-06-01T14:33:10.017963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Построение confusion matrix\ncm = confusion_matrix(test_labels, predicted_labels)\n#print(f'Confusion Matrix: \\n{cm}')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:10.020362Z","iopub.execute_input":"2024-06-01T14:33:10.020657Z","iopub.status.idle":"2024-06-01T14:33:10.027336Z","shell.execute_reply.started":"2024-06-01T14:33:10.020632Z","shell.execute_reply":"2024-06-01T14:33:10.026413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:10.028633Z","iopub.execute_input":"2024-06-01T14:33:10.029136Z","iopub.status.idle":"2024-06-01T14:33:10.036836Z","shell.execute_reply.started":"2024-06-01T14:33:10.029105Z","shell.execute_reply":"2024-06-01T14:33:10.036017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Визуализация матрицы ошибок с использованием seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=set(test_labels), yticklabels=set(predicted_labels))\nplt.xlabel('Предсказанный класс')\nplt.ylabel('Истинный класс')\nplt.title('Матрица ошибок')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:10.038130Z","iopub.execute_input":"2024-06-01T14:33:10.038521Z","iopub.status.idle":"2024-06-01T14:33:11.524795Z","shell.execute_reply.started":"2024-06-01T14:33:10.038485Z","shell.execute_reply":"2024-06-01T14:33:11.523776Z"},"trusted":true},"execution_count":null,"outputs":[]}]}