{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":585165,"sourceType":"datasetVersion","datasetId":284285}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-30T14:19:31.888753Z","iopub.execute_input":"2024-05-30T14:19:31.889517Z","iopub.status.idle":"2024-05-30T14:19:32.484738Z","shell.execute_reply.started":"2024-05-30T14:19:31.889450Z","shell.execute_reply":"2024-05-30T14:19:32.483438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, accuracy_score\nimport seaborn as sns\n\nnltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:32.487454Z","iopub.execute_input":"2024-05-30T14:19:32.488475Z","iopub.status.idle":"2024-05-30T14:19:36.283320Z","shell.execute_reply.started":"2024-05-30T14:19:32.488422Z","shell.execute_reply":"2024-05-30T14:19:36.281973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Путь к файлу CSV\ndata_path = \"/kaggle/input/atis-airlinetravelinformationsystem/atis_intents.csv\"\n\n# Чтение данных из CSV-файла\ndata = pd.read_csv(data_path)\ndata = data.rename(columns={'atis_flight': 'intent', ' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning': 'text'})\nlost_intent = {'intent': 'atis_flight', 'text': 'i want to fly from boston at 838 am and arrive in denver at 1110 in the morning'}\ndata = pd.concat([data, pd.DataFrame([lost_intent])], ignore_index=True)\n\n# Find classes with only one sample\nclass_counts = data['intent'].value_counts()\nsingle_sample_classes = class_counts[class_counts == 1].index.tolist()\n\n# Separate single sample classes\nsingle_sample_data = data[data['intent'].isin(single_sample_classes)]\nmultiple_sample_data = data[~data['intent'].isin(single_sample_classes)]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:36.284922Z","iopub.execute_input":"2024-05-30T14:19:36.285566Z","iopub.status.idle":"2024-05-30T14:19:36.321156Z","shell.execute_reply.started":"2024-05-30T14:19:36.285527Z","shell.execute_reply":"2024-05-30T14:19:36.319817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform stratified split on multiple sample data\ntrain_multiple, test_multiple = train_test_split(multiple_sample_data, test_size=0.2, random_state=42, stratify=multiple_sample_data['intent'])\n\n# Add single sample data to both train and test sets to ensure all classes are present\ntrain = pd.concat([train_multiple, single_sample_data]).reset_index(drop=True)\ntest = pd.concat([test_multiple, single_sample_data]).reset_index(drop=True)\n\n# Ensure no duplicates in train and test sets\ntrain = train.drop_duplicates().reset_index(drop=True)\ntest = test.drop_duplicates().reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:36.325168Z","iopub.execute_input":"2024-05-30T14:19:36.325797Z","iopub.status.idle":"2024-05-30T14:19:36.355470Z","shell.execute_reply.started":"2024-05-30T14:19:36.325740Z","shell.execute_reply":"2024-05-30T14:19:36.353917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing classes and ensure all classes are present\ntrain_classes = set(train['intent'])\ntest_classes = set(test['intent'])\nall_classes = set(data['intent'])\n\n# Find missing classes in train and test sets\nmissing_train_classes = all_classes - train_classes\nmissing_test_classes = all_classes - test_classes\n\n# Add missing classes examples to train and test sets\nif missing_train_classes:\n    missing_train_data = data[data['intent'].isin(missing_train_classes)]\n    train = pd.concat([train, missing_train_data])\n    train = train.drop_duplicates().reset_index(drop=True)\n\nif missing_test_classes:\n    missing_test_data = data[data['intent'].isin(missing_test_classes)]\n    test = pd.concat([test, missing_test_data])\n    test = test.drop_duplicates().reset_index(drop=True)\n\n# Print unique value proportions\nunique_values_normalized = train['intent'].value_counts(normalize=True)\nunique_values_normalized_test = test['intent'].value_counts(normalize=True)\nunique_values_normalized_data = data['intent'].value_counts(normalize=True)\nprint(unique_values_normalized * 100, len(unique_values_normalized), len(unique_values_normalized_test),len(unique_values_normalized_data)) ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:36.365321Z","iopub.execute_input":"2024-05-30T14:19:36.365796Z","iopub.status.idle":"2024-05-30T14:19:36.396258Z","shell.execute_reply.started":"2024-05-30T14:19:36.365751Z","shell.execute_reply":"2024-05-30T14:19:36.394504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding\nle = LabelEncoder()\nle.fit(data['intent'])  # Fit on the entire data to include all possible labels\ntrain['intent'] = le.transform(train['intent'])\ntest['intent'] = le.transform(test['intent'])\n\n# TF-IDF vectorization\nvectorizer = TfidfVectorizer(tokenizer=word_tokenize)\nX_train = vectorizer.fit_transform(train['text']).toarray()\nX_test = vectorizer.transform(test['text']).toarray()\n\ny_train = train['intent']\ny_test = test['intent']\n\n# Reshape data to add sequence dimension\nX_train = np.expand_dims(X_train, axis=1)\nX_test = np.expand_dims(X_test, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:36.397862Z","iopub.execute_input":"2024-05-30T14:19:36.399184Z","iopub.status.idle":"2024-05-30T14:19:37.568167Z","shell.execute_reply.started":"2024-05-30T14:19:36.399124Z","shell.execute_reply":"2024-05-30T14:19:37.566910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определение датасета\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = torch.tensor(texts, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return self.texts[idx], self.labels[idx]\n\n# Определение модели RNN\nclass RNNClassifier(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        rnn_out, _ = self.rnn(x)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:37.569786Z","iopub.execute_input":"2024-05-30T14:19:37.570569Z","iopub.status.idle":"2024-05-30T14:19:37.582632Z","shell.execute_reply.started":"2024-05-30T14:19:37.570524Z","shell.execute_reply":"2024-05-30T14:19:37.581146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TextDataset(X_train, y_train)\ntest_dataset = TextDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Инициализация модели\ninput_dim = X_train.shape[2]\nhidden_dim = 256\noutput_dim = len(le.classes_)\nmodel = RNNClassifier(input_dim, hidden_dim, output_dim)\n\n# Настройка устройства\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Определение функции потерь и оптимизатора\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:37.584564Z","iopub.execute_input":"2024-05-30T14:19:37.585647Z","iopub.status.idle":"2024-05-30T14:19:39.507191Z","shell.execute_reply.started":"2024-05-30T14:19:37.585584Z","shell.execute_reply":"2024-05-30T14:19:39.505945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Параметры обучения\nn_epoches = 15\n\n# Листы для хранения потерь на обучающей и валидационной выборках\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(n_epoches):\n    train_loss = 0\n    val_loss = 0\n\n    # Обучение\n    model.train()\n    for texts, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{n_epoches} - Training'):\n        texts = texts.to(device)\n        labels = labels.to(device)\n\n        outputs = model(texts)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    # Валидация\n    model.eval()\n    with torch.no_grad():\n        for texts, labels in tqdm(test_loader, desc=f'Epoch {epoch+1}/{n_epoches} - Validation'):\n            texts = texts.to(device)\n            labels = labels.to(device)\n\n            outputs = model(texts)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n    # Вычисление среднего значения потерь на эпоху\n    train_loss /= len(train_loader)\n    val_loss /= len(test_loader)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\n# Построение графика потерь\nplt.plot(range(1, n_epoches+1), train_losses, label='Train Loss')\nplt.plot(range(1, n_epoches+1), val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:39.512171Z","iopub.execute_input":"2024-05-30T14:19:39.513573Z","iopub.status.idle":"2024-05-30T14:19:48.987808Z","shell.execute_reply.started":"2024-05-30T14:19:39.513508Z","shell.execute_reply":"2024-05-30T14:19:48.986769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timeit","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:48.989295Z","iopub.execute_input":"2024-05-30T14:19:48.989938Z","iopub.status.idle":"2024-05-30T14:19:48.995036Z","shell.execute_reply.started":"2024-05-30T14:19:48.989900Z","shell.execute_reply":"2024-05-30T14:19:48.993720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions on the test set\npredictions = []\ntrue_labels = []\nstart_test = timeit.default_timer()\nmodel.eval()\nwith torch.no_grad():\n    for texts, labels in test_loader:\n        texts = texts.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(texts)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        predictions.extend(predicted.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\nend_test = timeit.default_timer()\n\n# Translate predicted labels back to original intents\npredicted_intents = le.inverse_transform(predictions)\n\n# Total number of predictions\nnum_predictions = len(predictions)\n\n# Average time per response\naverage_time_per_response = (end_test - start_test) / num_predictions\n\nprint(f'Time for testing: {end_test - start_test:.4f} seconds')\nprint(f'Average time per response: {average_time_per_response:.6f} seconds')\n\n# Model evaluation\nbalanced_acc = balanced_accuracy_score(true_labels, predictions)\nprecision, recall, fscore, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n\nprint(f'Balanced Accuracy: {balanced_acc:.4f}')\nprint(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {fscore:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T14:19:48.997111Z","iopub.execute_input":"2024-05-30T14:19:48.997974Z","iopub.status.idle":"2024-05-30T14:19:49.074473Z","shell.execute_reply.started":"2024-05-30T14:19:48.997924Z","shell.execute_reply":"2024-05-30T14:19:49.073444Z"},"trusted":true},"execution_count":null,"outputs":[]}]}